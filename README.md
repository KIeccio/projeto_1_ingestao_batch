# Ingestao de dados transacionais em batch via Airflow

Com o objetivo de demonstrar uma arquitetura medalh√£o (source ‚Üí bronze ‚Üí silver), este projeto realiza a coleta de dados transacionais di√°rios. O pipeline completo √© executado localmente, com a orquestra√ß√£o do Apache Airflow e o processamento de dados gerenciado pelo Apache Spark e Delta Lake.

>>OBS: foi escolhido PySpark por quest√µes acad√©micas. Em um contexto real, a massa de dados precisaria ser maior para justificar a necessidade da tecnologia. Mas, para utiliza√ß√£o junto ao Delta, o PySpark foi a escolha mais simples.

## üìÇ 1. Estrutura do Projeto

```
project-root/
‚îÇ
‚îú‚îÄ‚îÄ mock_data/
‚îÇ   ‚îî‚îÄ‚îÄ MOCK_DATA.csv              # Base de dados completa usada como origem
‚îÇ
‚îú‚îÄ‚îÄ datalake/                      # Pasta externa contendo os dados organizados por camadas. Voc√™ precisa criar essa pasta e apontar na vari√°vel "DATALAKE_PATH" dentro dos arquivos
‚îÇ   ‚îú‚îÄ‚îÄ source/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transaction_system/    # Arquivos CSV di√°rios de entrada
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transaction_data/      # Dados no formato Delta Lake (raw zone)
‚îÇ   ‚îî‚îÄ‚îÄ silver/
‚îÇ       ‚îî‚îÄ‚îÄ transaction_data/      # Dados tratados e deduplicados no formato Delta Lake
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ ingest_transaction_data.py # DAG do Airflow respons√°vel pela ingest√£o di√°ria. Voc√™ pode utilizar o comando 'cp' para copiar essa DAG para sua pasta de DAGs do Airflow.
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ mock_data_spliter.py       # Divide o MOCK_DATA.csv em 10 partes com datas diferentes
‚îÇ   ‚îú‚îÄ‚îÄ ingest_source_to_bronze.py # Realiza ingest√£o dos arquivos CSV para camada bronze
‚îÇ   ‚îî‚îÄ‚îÄ ingest_bronze_to_silver.py # Atualiza a camada silver com dados incrementais da bronze
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt               # Bibliotecas necess√°rias para execu√ß√£o
‚îî‚îÄ‚îÄ README.md                      # Este arquivo
```

> ‚ö†Ô∏è Certifique-se de criar manualmente a pasta `datalake/` na raiz do projeto antes da execu√ß√£o e apontar o caminho absoluto nas respectivas vari√°veis.


## üß∞ 2. Tech Stack:

- **Linguagem:** Python
- **Processamento:** PySpark, Pandas
- **Orquestra√ß√£o:** Apache Airflow
- **Data Lake & Arquitetura:** Medallion Architecture, Delta Lake
- **Armazenamento:** Parquet
- **Gera√ß√£o de dados:** Mockaroo

## üß† 3. Racioc√≠nio:

1. Gera√ß√£o dos dados: 
O processo come√ßou com a gera√ß√£o de dados fict√≠cios utilizando o site _Mockaroo_. Foi criado um arquivo CSV com 1.000 linhas com tr√™s colunas principais: transaction_id, customer_id e amount. 

2. Organiza√ß√£o e ingest√£o inicial:
Os arquivos CSV foram armazenados em uma pasta **source**, que serviu como fonte de dados. Em seguida, os dados foram ingeridos em dez partes distintas, cada uma com um **timestamp** associado, possibilitando um processamento din√¢mico por meio do **Airflow**.

3. Arquitetura do Data Lake:
Criei um reposit√≥rio que atuou como **Data Lake**, seguindo a **arquitetura medallion**, composta pelas camadas: Bronze e Silver. A ingest√£o ocorreu da seguinte forma: 
- **Source ‚Üí Bronze**: os dados foram ingeridos em formato parquet. 
- **Bronze ‚Üí Silver**: os dados foram transformados em uma **tabela Delta**, preparando-os para uma futura camada **Gold**.

Neste ponto, n√£o foram aplicadas regras de neg√≥cio, mas em projetos reais costuma-se incluir processos como: Exclus√£o de dados nulos, valida√ß√µes via regex e padroniza√ß√µes diversas.

4. Considera√ß√µes sobre formatos:
Algumas literaturas defendem que a camada **Bronze** deve manter os arquivos exatamente como na fonte original (por exemplo, CSV ‚Üí CSV). Outras abordagens sugerem transformar os arquivos originais em uma **tabela Delta** com estrat√©gia _append only_, deixando a etapa de processamento para a camada **Silver**.

Ambas as pr√°ticas possuem **vantagens e desvantagens** e a escolha depende da **metodologia** ou **requisitos do projeto**.

## üë©üèΩ‚Äçüíª4. Como reproduzir cada etapa passo a passo:


Esta se√ß√£o detalha os passos necess√°rios para replicar a arquitetura de ingest√£o de dados em sua pr√≥pria m√°quina. Certifique-se de ter todos os pr√©-requisitos instalados e configurados antes de come√ßar.

A ideia √© que qualquer pessoa consiga seguir o guia e replicar a sua arquitetura de ingest√£o de dados.

### 1. Pr√©-requisitos e Configura√ß√£o Inicial:

Para rodar este projeto, voc√™ precisar√° de um ambiente com Python e o Apache Airflow j√° configurado. Se voc√™ ainda n√£o tem o Airflow rodando, siga a documenta√ß√£o oficial para configur√°-lo.

- Verifique se o Airflow est√° rodando: Certifique-se de que o seu ambiente do Airflow est√° acess√≠vel e operacional.

- Crie a estrutura de pastas do Data Lake: No diret√≥rio raiz do seu projeto, crie uma pasta que servir√° como o seu Data Lake. Voc√™ pode cham√°-la de `data_lake`. Dentro dela, crie as subpastas que representam as camadas da arquitetura medallion:

- data_lake/bronze
- data_lake/silver
- data_lake/source

### 2. Gera√ß√£o e Prepara√ß√£o dos Dados:

A sua arquitetura depende de dados transacionais. Siga os passos abaixo para gerar e preparar os arquivos de origem:

- Gerar os dados: Acesse o site [Mockaroo](https://www.mockaroo.com/) e crie um arquivo CSV com 1.000 linhas, usando as colunas transaction_id (Row Number), customer_id (Number) e amount (Number).

- Dividir os dados: Divida o arquivo CSV original em dez arquivos menores e realize a ingest√£o na pasta `source`. Nomeie-os de forma sequencial utilizando timestamp (por exemplo, transacoes_dia_2025-09-04.csv, transacoes_dia_2025-09-03.csv, transacoes_dia_2025-09-02 etc.). No meu projeto eu realizei essa ingest√£o no diret√≥rio `transaction_system`.

### 3. Cria√ß√£o do DAG e L√≥gica de Ingest√£o

Agora, voc√™ precisa criar o Directed Acyclic Graph (DAG) que orquestrar√° as etapas de ingest√£o.

- **Crie o arquivo do DAG:** Dentro da pasta `dags` do seu projeto Airflow, crie um arquivo Python, por exemplo, `dag_medallion.py`.

- **Escreva o c√≥digo:** O c√≥digo do DAG deve conter os `tasks` (tarefas) para cada etapa do seu projeto:
    
    - **Task 1: Source -> Bronze:** Uma tarefa que l√™ os arquivos CSV da pasta `source`, converte-os para o formato Parquet e os armazena na pasta `bronze`. 
    
    - **Task 2: Bronze -> Silver:** Uma tarefa que l√™ os arquivos Parquet da camada `bronze`, cria um Delta Table e armazena os dados na pasta `silver`. Esta √© a etapa ideal para futuras transforma√ß√µes, mas neste projeto inicial, ser√° apenas uma convers√£o de formato.
    

Ao executar o DAG, ele ler√° os arquivos, processar√°-os e mover√° os dados entre as camadas, simulando o fluxo de uma arquitetura de dados real.

√â importante que as vari√°veis de ambientes estejam previamente configuradas no seu c√≥digo, elas devem apontar para os seus respectivos diret√≥rios no Data Lake: 

1. DATALAKE_PATH = `'/caminho/para/seu/projeto/datalake'`
2. SOURCE_PATH = `f"{DATALAKE_PATH}'/source/transaction_system/{arquivo}.csv'"`
3. BRONZE_PATH = `f"{DATALAKE_PATH}'/bronze/transaction_data'"`
4. SILVER_PATH = `f"{DATALAKE_PATH}'/silver/transaction_data'"`

## 5. Execu√ß√£o da DAG no Airflow

Com o c√≥digo da DAG criado e o seu ambiente do Airflow configurado, a √∫ltima etapa √© orquestrar o processo. O Airflow vai detectar o seu novo arquivo de DAG e mostr√°-lo na interface web, permitindo que voc√™ o execute.

### Passo a Passo

1. **Acessar a Interface do Airflow**: Abra seu navegador e navegue at√© a URL da sua interface web do Airflow. Geralmente, √© `http://localhost:8080`.

2. **Verificar a DAG**: Na p√°gina principal, voc√™ dever√° ver a sua DAG, nomeada de acordo com o `dag_id` definido no seu arquivo Python (por exemplo, `dag_medallion`). Por padr√£o, a DAG √© criada desabilitada.

3. **Habilitar a DAG**: Clique no bot√£o para habilitar a DAG (geralmente um bot√£o de "liga/desliga" ou um slider). Isso far√° com que o Airflow passe a monitorar a DAG e a execute de acordo com o seu `schedule_interval`.

4. **Executar a DAG Manualmente (Opcional)**: Para testar a pipeline imediatamente, voc√™ pode acionar a execu√ß√£o manualmente. Na interface da DAG, clique no bot√£o de "play" e selecione "Trigger DAG". Isso iniciar√° uma nova execu√ß√£o imediatamente.

5. **Monitorar a Execu√ß√£o**: Ap√≥s acionar a DAG, voc√™ pode acompanhar o progresso das tarefas clicando nela. A visualiza√ß√£o **Graph View** ou **Gantt Chart** s√£o excelentes para ver o status de cada tarefa (se est√° rodando, teve sucesso ou falhou).

Ao finalizar a execu√ß√£o com sucesso, voc√™ ver√° os resultados no seu **data lake**.

- A pasta `data_lake/bronze` conter√° os arquivos transformados no formato **Delta** (ou Parquet, dependendo da sua implementa√ß√£o) para cada dia de processamento.

- A pasta `data_lake/silver` conter√° a tabela **Delta** limpa e atualizada, pronta para ser consumida pela camada Gold.
