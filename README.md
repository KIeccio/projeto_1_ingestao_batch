# Ingestao de dados transacionais em batch via Airflow

Com o objetivo de demonstrar uma arquitetura medalh√£o (source ‚Üí bronze ‚Üí silver), este projeto realiza a coleta de dados transacionais di√°rios. O pipeline completo √© executado localmente, com a orquestra√ß√£o do Apache Airflow e o processamento de dados gerenciado pelo Apache Spark e Delta Lake.

>>OBS: foi escolhido PySpark por quest√µes acad√©micas. Em um contexto real, a massa de dados precisaria ser maior para justificar a necessidade da tecnologia. Mas, para utiliza√ß√£o junto ao Delta, o PySpark foi a escolha mais simples.

## üìÇ 1. Estrutura do Projeto

```
project-root/
‚îÇ
‚îú‚îÄ‚îÄ mock_data/
‚îÇ   ‚îî‚îÄ‚îÄ MOCK_DATA.csv              # Base de dados completa usada como origem
‚îÇ
‚îú‚îÄ‚îÄ datalake/                      # Pasta externa contendo os dados organizados por camadas. Voc√™ precisa criar essa pasta e apontar na vari√°vel "DATALAKE_PATH" dentro dos arquivos
‚îÇ   ‚îú‚îÄ‚îÄ source/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transaction_system/    # Arquivos CSV di√°rios de entrada
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transaction_data/      # Dados no formato Delta Lake (raw zone)
‚îÇ   ‚îî‚îÄ‚îÄ silver/
‚îÇ       ‚îî‚îÄ‚îÄ transaction_data/      # Dados tratados e deduplicados no formato Delta Lake
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ ingest_transaction_data.py # DAG do Airflow respons√°vel pela ingest√£o di√°ria. Voc√™ pode utilizar o comando 'cp' para copiar essa DAG para sua pasta de DAGs do Airflow.
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ mock_data_spliter.py       # Divide o MOCK_DATA.csv em 10 partes com datas diferentes
‚îÇ   ‚îú‚îÄ‚îÄ ingest_source_to_bronze.py # Realiza ingest√£o dos arquivos CSV para camada bronze
‚îÇ   ‚îî‚îÄ‚îÄ ingest_bronze_to_silver.py # Atualiza a camada silver com dados incrementais da bronze
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt               # Bibliotecas necess√°rias para execu√ß√£o
‚îî‚îÄ‚îÄ README.md                      # Este arquivo
```

> ‚ö†Ô∏è Certifique-se de criar manualmente a pasta `datalake/` na raiz do projeto antes da execu√ß√£o e apontar o caminho absoluto nas respectivas vari√°veis.


## üß∞ 2. Tech Stack:

- **Linguagem:** Python
- **Processamento:** PySpark, Pandas
- **Orquestra√ß√£o:** Apache Airflow
- **Data Lake & Arquitetura:** Medallion Architecture, Delta Lake
- **Armazenamento:** Parquet
- **Gera√ß√£o de dados:** Mockaroo

## üß† 3. Racioc√≠nio:

1. Gera√ß√£o dos dados: 
O processo come√ßou com a gera√ß√£o de dados fict√≠cios utilizando o site _Mockaroo_. Foi criado um arquivo CSV com 1.000 linhas com tr√™s colunas principais: transaction_id, customer_id e amount. 

2. Organiza√ß√£o e ingest√£o inicial:
Os arquivos CSV foram armazenados em uma pasta **source**, que serviu como fonte de dados. Em seguida, os dados foram ingeridos em dez partes distintas, cada uma com um **timestamp** associado, possibilitando um processamento din√¢mico por meio do **Airflow**.

3. Arquitetura do Data Lake:
Criei um reposit√≥rio que atuou como **Data Lake**, seguindo a **arquitetura medallion**, composta pelas camadas: Bronze e Silver. A ingest√£o ocorreu da seguinte forma: 
- **Source ‚Üí Bronze**: os dados foram ingeridos em formato parquet. 
- **Bronze ‚Üí Silver**: os dados foram transformados em uma **tabela Delta**, preparando-os para uma futura camada **Gold**.

Neste ponto, n√£o foram aplicadas regras de neg√≥cio, mas em projetos reais costuma-se incluir processos como: Exclus√£o de dados nulos, valida√ß√µes via regex e padroniza√ß√µes diversas.

4. Considera√ß√µes sobre formatos:
Algumas literaturas defendem que a camada **Bronze** deve manter os arquivos exatamente como na fonte original (por exemplo, CSV ‚Üí CSV). Outras abordagens sugerem transformar os arquivos originais em uma **tabela Delta** com estrat√©gia _append only_, deixando a etapa de processamento para a camada **Silver**.

Ambas as pr√°ticas possuem **vantagens e desvantagens** e a escolha depende da **metodologia** ou **requisitos do projeto**.

## üë©üèΩ‚Äçüíª4. Como reproduzir cada etapa passo a passo:

### 1. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

### 2. Gere os arquivos simulados
```bash
python scripts/mock_data_spliter.py
```

### 3. Inicie o Airflow standalone
```bash
airflow standalone
```

### 4. Ative a DAG no painel do Airflow
Acesse o Airflow em [http://localhost:8080](http://localhost:8080) e ative a DAG `INGEST_TRANSACTION_DATA`.

> A DAG est√° agendada para rodar todos os dias √†s 03:00 (`0 3 * * *`), com suporte a **catchup** de dias anteriores. A simula√ß√£o inclui falhas nos dias em que n√£o existem arquivos.

